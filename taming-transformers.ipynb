{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  params:\n",
      "    batch_size: 1\n",
      "    validation:\n",
      "      target: taming.data.sflckr.Examples\n",
      "  target: main.DataModuleFromConfig\n",
      "model:\n",
      "  base_learning_rate: 4.5e-06\n",
      "  params:\n",
      "    cond_stage_config:\n",
      "      params:\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 182\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 182\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        image_key: segmentation\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.VQModel\n",
      "    cond_stage_key: segmentation\n",
      "    first_stage_config:\n",
      "      params:\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 3\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 3\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.VQModel\n",
      "    first_stage_key: image\n",
      "    transformer_config:\n",
      "      params:\n",
      "        block_size: 512\n",
      "        n_embd: 1024\n",
      "        n_head: 16\n",
      "        n_layer: 24\n",
      "        vocab_size: 1024\n",
      "      target: taming.modules.transformer.mingpt.GPT\n",
      "  target: taming.models.cond_transformer.Net2NetTransformer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "config_path = \"logs/2020-11-09T13-31-51_sflckr/configs/2020-11-09T13-31-51-project.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "import yaml\n",
    "print(yaml.dump(OmegaConf.to_container(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    }
   ],
   "source": [
    "from taming.models.cond_transformer import Net2NetTransformer\n",
    "model = Net2NetTransformer(**config.model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"logs/2020-11-09T13-31-51_sflckr/checkpoints/last.ckpt\"\n",
    "sd = torch.load(ckpt_path, map_location=device)[\"state_dict\"]\n",
    "# sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 3754.84 MiB\n",
      "Cached memory: 3812.00 MiB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MiB\")\n",
    "print(f\"Cached memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    3754 MB |    3754 MB |    3754 MB |       0 B  |\n",
      "|       from large pool |    3645 MB |    3645 MB |    3645 MB |       0 B  |\n",
      "|       from small pool |     109 MB |     109 MB |     109 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    3754 MB |    3754 MB |    3754 MB |       0 B  |\n",
      "|       from large pool |    3645 MB |    3645 MB |    3645 MB |       0 B  |\n",
      "|       from small pool |     109 MB |     109 MB |     109 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    3812 MB |    3812 MB |    3812 MB |       0 B  |\n",
      "|       from large pool |    3700 MB |    3700 MB |    3700 MB |       0 B  |\n",
      "|       from small pool |     112 MB |     112 MB |     112 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   58533 KB |   64565 KB |    1475 MB |    1418 MB |\n",
      "|       from large pool |   56320 KB |   60416 KB |    1410 MB |    1355 MB |\n",
      "|       from small pool |    2213 KB |    4684 KB |      65 MB |      62 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1772    |    1772    |    1772    |       0    |\n",
      "|       from large pool |     495    |     495    |     495    |       0    |\n",
      "|       from small pool |    1277    |    1277    |    1277    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1772    |    1772    |    1772    |       0    |\n",
      "|       from large pool |     495    |     495    |     495    |       0    |\n",
      "|       from small pool |    1277    |    1277    |    1277    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     264    |     264    |     264    |       0    |\n",
      "|       from large pool |     208    |     208    |     208    |       0    |\n",
      "|       from small pool |      56    |      56    |      56    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      31    |      35    |     149    |     118    |\n",
      "|       from large pool |      22    |      22    |      93    |      71    |\n",
      "|       from small pool |       9    |      13    |      56    |      47    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.68 GiB total capacity; 3.70 GiB already allocated; 21.38 MiB free; 3.75 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# !pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/taming/lib/python3.8/site-packages/pytorch_lightning/utilities/device_dtype_mixin.py:124\u001b[0m, in \u001b[0;36mDeviceDtypeModuleMixin.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03mThis also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03mit should be called before constructing optimizer if the module will\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    Module: self\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__update_properties(device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/taming/lib/python3.8/site-packages/torch/nn/modules/module.py:637\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    621\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/taming/lib/python3.8/site-packages/torch/nn/modules/module.py:530\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 530\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    535\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/taming/lib/python3.8/site-packages/torch/nn/modules/module.py:530\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 530\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    535\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 530 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/taming/lib/python3.8/site-packages/torch/nn/modules/module.py:530\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 530\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    535\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    541\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/taming/lib/python3.8/site-packages/torch/nn/modules/module.py:552\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 552\u001b[0m         param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/taming/lib/python3.8/site-packages/torch/nn/modules/module.py:637\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    621\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.68 GiB total capacity; 3.70 GiB already allocated; 21.38 MiB free; 3.75 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# !pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "model.cuda().eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 486.00 MiB (GPU 0; 5.68 GiB total capacity; 3.70 GiB already allocated; 32.00 MiB free; 3.75 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(segmentation)\n\u001b[1;32m      6\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m182\u001b[39m)[segmentation]\n\u001b[0;32m----> 7\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 486.00 MiB (GPU 0; 5.68 GiB total capacity; 3.70 GiB already allocated; 32.00 MiB free; 3.75 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "segmentation_path = \"data/sflckr_segmentations/norway/25735082181_999927fe5a_b.png\"\n",
    "segmentation = Image.open(segmentation_path)\n",
    "segmentation = np.array(segmentation)\n",
    "segmentation = np.eye(182)[segmentation]\n",
    "segmentation = torch.tensor(segmentation.transpose(2,0,1)[None]).to(dtype=torch.float32, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_segmentation(s):\n",
    "  s = s.detach().cpu().numpy().transpose(0,2,3,1)[0,:,:,None,:]\n",
    "  colorize = np.random.RandomState(1).randn(1,1,s.shape[-1],3)\n",
    "  colorize = colorize / colorize.sum(axis=2, keepdims=True)\n",
    "  s = s@colorize\n",
    "  s = s[...,0,:]\n",
    "  s = ((s+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
    "  s = Image.fromarray(s)\n",
    "  display(s)\n",
    "\n",
    "show_segmentation(segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_code, c_indices = model.encode_to_c(segmentation)\n",
    "print(\"c_code\", c_code.shape, c_code.dtype)\n",
    "print(\"c_indices\", c_indices.shape, c_indices.dtype)\n",
    "assert c_code.shape[2]*c_code.shape[3] == c_indices.shape[0]\n",
    "segmentation_rec = model.cond_stage_model.decode(c_code)\n",
    "show_segmentation(torch.softmax(segmentation_rec, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(s):\n",
    "  s = s.detach().cpu().numpy().transpose(0,2,3,1)[0]\n",
    "  s = ((s+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
    "  s = Image.fromarray(s)\n",
    "  display(s)\n",
    "\n",
    "codebook_size = config.model.params.first_stage_config.params.embed_dim\n",
    "z_indices_shape = c_indices.shape\n",
    "z_code_shape = c_code.shape\n",
    "z_indices = torch.randint(codebook_size, z_indices_shape, device=model.device)\n",
    "x_sample = model.decode_to_img(z_indices, z_code_shape)\n",
    "show_image(x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "idx = z_indices\n",
    "idx = idx.reshape(z_code_shape[0],z_code_shape[2],z_code_shape[3])\n",
    "\n",
    "cidx = c_indices\n",
    "cidx = cidx.reshape(c_code.shape[0],c_code.shape[2],c_code.shape[3])\n",
    "\n",
    "temperature = 1.0\n",
    "top_k = 100\n",
    "update_every = 50\n",
    "\n",
    "start_t = time.time()\n",
    "for i in range(0, z_code_shape[2]-0):\n",
    "  if i <= 8:\n",
    "    local_i = i\n",
    "  elif z_code_shape[2]-i < 8:\n",
    "    local_i = 16-(z_code_shape[2]-i)\n",
    "  else:\n",
    "    local_i = 8\n",
    "  for j in range(0,z_code_shape[3]-0):\n",
    "    if j <= 8:\n",
    "      local_j = j\n",
    "    elif z_code_shape[3]-j < 8:\n",
    "      local_j = 16-(z_code_shape[3]-j)\n",
    "    else:\n",
    "      local_j = 8\n",
    "\n",
    "    i_start = i-local_i\n",
    "    i_end = i_start+16\n",
    "    j_start = j-local_j\n",
    "    j_end = j_start+16\n",
    "\n",
    "    patch = idx[:,i_start:i_end,j_start:j_end]\n",
    "    patch = patch.reshape(patch.shape[0],-1)\n",
    "    cpatch = cidx[:, i_start:i_end, j_start:j_end]\n",
    "    cpatch = cpatch.reshape(cpatch.shape[0], -1)\n",
    "    patch = torch.cat((cpatch, patch), dim=1)\n",
    "    logits,_ = model.transformer(patch[:,:-1])\n",
    "    logits = logits[:, -256:, :]\n",
    "    logits = logits.reshape(z_code_shape[0],16,16,-1)\n",
    "    logits = logits[:,local_i,local_j,:]\n",
    "\n",
    "    logits = logits/temperature\n",
    "\n",
    "    if top_k is not None:\n",
    "      logits = model.top_k_logits(logits, top_k)\n",
    "\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    idx[:,i,j] = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    step = i*z_code_shape[3]+j\n",
    "    if step%update_every==0 or step==z_code_shape[2]*z_code_shape[3]-1:\n",
    "      x_sample = model.decode_to_img(idx, z_code_shape)\n",
    "      clear_output()\n",
    "      print(f\"Time: {time.time() - start_t} seconds\")\n",
    "      print(f\"Step: ({i},{j}) | Local: ({local_i},{local_j}) | Crop: ({i_start}:{i_end},{j_start}:{j_end})\")\n",
    "      show_image(x_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
